\subsection{Hypotheses}
For feed-forward networks, we hypothesize that their performance will decrease as the number of their hidden layers increases. For radial basis function networks, we believe that condensed will provide the best performance, k-medoids will provide the second best performance, and k-means will result in the worst performance of the three. Further we hypothesize that for each dataset we will see a significant difference between the best of each type of model (i.e. the highest performing feed forward variation will be significantly better then the highest performing RBF model, or vice versa).